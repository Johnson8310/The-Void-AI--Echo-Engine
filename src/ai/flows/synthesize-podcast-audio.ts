
'use server';
/**
 * @fileOverview Generates the podcast audio file from the edited script using the selected AI voices.
 *
 * - synthesizePodcastAudio - A function that handles the podcast audio synthesis process.
 * - SynthesizePodcastAudioInput - The input type for the synthesizePodcastAudio function.
 * - SynthesizePodcastAudioOutput - The return type for the synthesizePodcastAudioOutput function.
 */

import {z} from 'zod';
import {ai} from '@/ai/genkit';
import { googleAI } from '@genkit-ai/googleai';

const SynthesizePodcastAudioInputSchema = z.object({
  script: z.string().describe('The edited script with speaker cues.'),
  voiceConfig: z
    .record(z.string(), z.object({voiceName: z.string()}))
    .describe(
      'A map of speaker names to their AI voice names. A special `__default` key can be used for a single voice.'
    ),
});

export type SynthesizePodcastAudioInput = z.infer<
  typeof SynthesizePodcastAudioInputSchema
>;

const SynthesizePodcastAudioOutputSchema = z.object({
  podcastAudioUri: z
    .string()
    .describe(
      'The data URI of the generated podcast audio file in WAV format.'
    ),
});

export type SynthesizePodcastAudioOutput = z.infer<
  typeof SynthesizePodcastAudioOutputSchema
>;


function formatMultiSpeakerScript(script: string, speakers: string[]): string {
    const lines = script.split('\n');
    const formattedLines: string[] = [];
    const speakerRegex = new RegExp(`^(${speakers.join('|')}):\\s*(.+)`, 'i');

    for (const line of lines) {
        const match = line.match(speakerRegex);
        if (match) {
            const speaker = match[1].trim();
            const text = match[2].trim();
            if(text) {
              formattedLines.push(`${speaker}: ${text}`);
            }
        }
    }
    return formattedLines.join('\n');
}

export async function synthesizePodcastAudio(
  input: SynthesizePodcastAudioInput
): Promise<SynthesizePodcastAudioOutput> {
  const {script, voiceConfig} = input;

  const isMultiSpeaker = !voiceConfig['__default'];
  
  let ttsPrompt = script;
  const config: any = {
    responseModalities: ['AUDIO'],
    speechConfig: {},
  };

  if (isMultiSpeaker) {
    const speakers = Object.keys(voiceConfig);
    ttsPrompt = formatMultiSpeakerScript(script, speakers);
    config.speechConfig.multiSpeakerVoiceConfig = {
      speakerVoiceConfigs: speakers.map(speaker => ({
        speaker: speaker,
        voiceConfig: {
          prebuiltVoiceConfig: { voiceName: voiceConfig[speaker].voiceName },
        },
      })),
    };
  } else {
    config.speechConfig.voiceConfig = {
      prebuiltVoiceConfig: { voiceName: voiceConfig['__default'].voiceName },
    };
  }

  if (!ttsPrompt.trim()) {
    throw new Error(
      "The script is empty or could not be parsed into valid speaker segments. Please ensure the script has text to speak."
    );
  }

  try {
    const { media } = await ai.generate({
        model: googleAI.model('gemini-2.5-flash-preview-tts'),
        config,
        prompt: ttsPrompt,
    });

    if (!media) {
      throw new Error('No audio was generated by the model.');
    }
  
    return {podcastAudioUri: media.url};
  } catch (error: any) {
    console.error('Error with Google AI TTS API:', error);
    throw new Error(`Failed to synthesize audio with Google AI: ${error.message}`);
  }
}
